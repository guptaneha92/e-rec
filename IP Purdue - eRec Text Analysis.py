# -*- coding: utf-8 -*-
"""Copy of Avg Similarity scores based on Node name w2v

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A_z47JOtvuUKpmbmza91pxx5wKVQta3G
"""

!pip install boto3

import time
import boto3
import pandas as pd
import io

class QueryAthena:

    def __init__(self, query, database):
        self.database = database
        self.folder = ''
        self.bucket = 'ipamazon'  ##replace with your bucket name
        self.s3_output =  's3://ipamazon'##replace with your bucket path
        self.region_name = 'us-east-1'##replace with your region
        self.aws_access_key_id = "**********" ##replace with your aws key id
        self.aws_secret_access_key = "*********" ##replace with your aws access key
        self.query = query

    def load_conf(self, q):
        try:
            self.client = boto3.client('athena', 
                              region_name = self.region_name, 
                              aws_access_key_id = self.aws_access_key_id,
                              aws_secret_access_key= self.aws_secret_access_key)
            response = self.client.start_query_execution(
                QueryString = q,
                    QueryExecutionContext={
                    'Database': self.database
                    },
                    ResultConfiguration={
                    'OutputLocation': self.s3_output,
                    }
            )
            self.filename = response['QueryExecutionId']
            print('Execution ID: ' + response['QueryExecutionId'])

        except Exception as e:
            print(e)
        return response                

    def run_query(self):
        queries = [self.query]
        for q in queries:
            res = self.load_conf(q)
        try:              
            query_status = None
            while query_status == 'QUEUED' or query_status == 'RUNNING' or query_status is None:
                query_status = self.client.get_query_execution(QueryExecutionId=res["QueryExecutionId"])['QueryExecution']['Status']['State']
                print(query_status)
                if query_status == 'FAILED' or query_status == 'CANCELLED':
                    raise Exception('Athena query with the string "{}" failed or was cancelled'.format(self.query))
                time.sleep(10)
            print('Query "{}" finished.'.format(self.query))

            df = self.obtain_data()
            return df

        except Exception as e:
            print(e)      

    def obtain_data(self):
        try:
            self.resource = boto3.resource('s3', 
                                  region_name = self.region_name, 
                                  aws_access_key_id = self.aws_access_key_id,
                                  aws_secret_access_key= self.aws_secret_access_key)

            response = self.resource.Bucket(self.bucket).Object(key= self.folder + self.filename + '.csv').get()
            tdf=pd.read_csv(io.BytesIO(response['Body'].read()), encoding='utf8') 
            self.resource.Bucket(self.bucket).Object(key= self.folder + self.filename + '.csv').delete()
            self.resource.Bucket(self.bucket).Object(key= self.folder + self.filename + '.csv.metadata').delete()
            return tdf   
        except Exception as e:
            print(e)  


if __name__ == "__main__":       
    query = '''SELECT  
array_join(transform(filter(t.brand, e -> e.language_tag like 'en%'), x -> x.value),'') as brand,
array_join(transform(filter(t.bullet_point, e -> e.language_tag like 'en%'), x -> x.value), '/') as bullet_point,
array_join(transform(filter(t.color, e -> e.language_tag like 'en%'), x -> x.value), '**') as color,
array_join(color_code, '**') as color_code,
t.country,
t.domain_name,
array_join(transform(filter(t.fabric_type, e -> e.language_tag like 'en%'), x -> x.value), '**') as fabric_type,
array_join(transform(filter(t.finish_type, e -> e.language_tag like 'en%'), x -> x.value), '**') as finish_type,
item_dimensions.height.normalized_value.value as item_height,
item_dimensions.length.normalized_value.value as item_length,
item_dimensions.width.normalized_value.value as item_width,
t.item_id,
array_join(transform(filter(t.item_keywords, e -> e.language_tag like 'en%'), x -> x.value), '**') as item_keywords,
array_join(transform(filter(t.item_name, e -> e.language_tag like 'en%'), x -> x.value),'') as item_name,
array_join(transform(filter(t.item_shape, e -> e.language_tag like 'en%'), x -> x.value),'') as item_shape,
array_join(transform(t.item_weight,x -> x.normalized_value.value),'') as item_weight,
t.main_image_id,
t.marketplace,
array_join(transform(filter(t.material, e -> e.language_tag like 'en%'), x -> x.value),'') as material,
array_join(transform(filter(t.model_name, e -> e.language_tag like 'en%'), x -> x.value),'') as model_name,
array_join(transform(t.model_number,x -> x.value),'') as model_number,
array_join(transform(t.model_year,x -> x.value),'') as model_year,
array_join(transform(t.node,x -> x.node_id),' , ') as node_id,
array_join(transform(t.node,x -> x.node_name),' , ') as node_name,
array_join(t.other_image_id,' , ') as other_image_id,
array_join(transform(filter(t.pattern, e -> e.language_tag like 'en%'), x -> x.value),'') as pattern,
array_join(transform(filter(t.product_description, e -> e.language_tag like 'en%'), x -> x.value), '**') as product_description,
array_join(transform(t.product_type,x -> x.value),' , ') as product_type,
t.spin_id,
array_join(transform(filter(t.style, e -> e.language_tag like 'en%'), x -> x.value),'') as style,
t."3dmodel_id" as "3dmodel_id"

FROM "default"."abo_listings" t '''
    qa = QueryAthena(query=query, database='default')
    dataframe = qa.run_query()

# not needed if you are running the code above
import pandas as pd
from google.colab import drive
import os
drive.mount('/content/drive')
os.chdir("/content/drive/My Drive/")
dataframe=pd.read_csv("Amazon Berkeley Dataset Output.csv")

dataframe.columns

dataframe.head()

!pip install missingno

#import missingno
#missingno.bar(dataframe[dataframe['domain_name']=="amazon.com"],figsize=(12,5))

dataframe[dataframe['domain_name']=="amazon.com"].isna().sum()

dataframe[dataframe['domain_name']=="amazon.com"]['product_type'].nunique()

import warnings
warnings.filterwarnings("ignore")

!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-mpnet-base-v2')
# model = SentenceTransformer('paraphrase-distilroberta-base-v1')

import gensim
from gensim.models import Word2Vec,KeyedVectors
model_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'
w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)

df=dataframe[dataframe['domain_name']=="amazon.com"]
df = df.iloc[: , 0:]
df.reset_index(drop=True,inplace=True)
df.head(5)

import numpy as np
bullets = df['bullet_point'].tolist()
# print(descriptions)
bp_emb = []
for i,bp in enumerate(bullets):
    bp_emb.append(model.encode(str(bp)))

import torch
from sentence_transformers import SentenceTransformer, util
def recommend(query,num):
    #Compute cosine-similarities with all embeddings 
    query_embedd = model.encode(query)
    torch.Argument 
    cosine_score = util.pytorch_cos_sim(query_embedd, bp_emb)
    cosine_scores=cosine_score.tolist()[0][:num+1]

    return cosine_scores
#id='B07358TXKD'
id="B075X4N56D"
# id = 'B07V38TBKP'
#id='B07YPTDRLV'
# id='B072M1WJ8S'
num=df.shape[0] 
query_show_des = df.loc[df['item_id'] == id]['bullet_point'].to_list()[0]
#recommendded_results,cosine_scores = recommend(query_show_des,num) 
cosine_scores = recommend(query_show_des,num)
#print(len(cosine_scores))
df_out=df
# for index in recommendded_results:
#     print(df.iloc[index,:])

df_out["cosine_scores_bulletpoint"]=cosine_scores

df_out.index.name = 'Old Index'
df_out=df_out.reset_index()
df_out=df_out[df_out["item_id"]!=id]
df[["item_id","item_name","item_keywords","node_name","bullet_point"]].loc[df['item_id'] == id]
# df[["item_id","item_name","node_name","bullet_point"]].loc[df['item_id'] == id]

df_out_sort=df_out[["item_id","item_name","brand","bullet_point","cosine_scores_bulletpoint"]].sort_values(by=['cosine_scores_bulletpoint'], ascending=False)
df_out_sort.head(100)
# df_out[["Rank","item_id","item_name","brand","node_name","product_type","cosine_scores","bullet_point"]]
# df_out[["item_id","item_name","brand","node_name","product_type","cosine_scores","item_keywords","bullet_point"]]

#import seaborn as sns
#sns.set(rc={"figure.figsize":(12, 8),'axes.titlesize':20,'axes.titleweight':'bold'}) 
#ax=sns.lineplot(data=df_out,x="Rank", y="cosine_scores",marker= 'o', markersize=6)
#ax.set_xlabel ("Similarity Rank")
#ax.set_ylabel ("Cosine Score")
#ax.set_title ("Recommendation Rank vs Similarity Score")



import numpy as np


class DocSim:
    def __init__(self, w2v_model, stopwords=None):
        self.w2v_model = w2v_model
        self.stopwords = stopwords if stopwords is not None else []

    def vectorize(self, doc: str) -> np.ndarray:
        """
        Identify the vector values for each word in the given document
        :param doc:
        :return:
        """
        doc = str(doc).lower()
        words = [w for w in doc.split(" ") if w not in self.stopwords]
        word_vecs = []
        for word in words:
            try:
                vec = self.w2v_model[word]
                word_vecs.append(vec)
            except KeyError:
                # Ignore, if the word doesn't exist in the vocabulary
                pass

        # Assuming that document vector is the mean of all the word vectors
        # PS: There are other & better ways to do it.
        vector = np.mean(word_vecs, axis=0)
        return vector

    def _cosine_sim(self, vecA, vecB):
        """Find the cosine similarity distance between two vectors."""
        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))
        if np.isnan(np.sum(csim)):
            return 0
        return csim

    def calculate_similarity(self, source_doc, target_docs=None, threshold=0):
        """Calculates & returns similarity scores between given source document & all
        the target documents."""
        if not target_docs:
            return []

        if isinstance(target_docs, str):
            target_docs = [target_docs]

        source_vec = self.vectorize(source_doc)
        results = []
        for doc in target_docs:
            
            target_vec = self.vectorize(doc)
            sim_score = self._cosine_sim(source_vec, target_vec)
            results.append(sim_score)
            # if sim_score > threshold:
                # results.append({"score": sim_score, "doc": doc})
                # results.append(sim_score)
            # Sort results by score in desc order
            # results.sort(key=lambda k: k["score"], reverse=True)

        return results

ds = DocSim(w2v_model)

source1 = df["item_keywords"].loc[df['item_id'] == id].to_list()[0]
tdf1=df_out["item_keywords"].fillna('NA')
target1 = tdf1.to_list()
#print(target1)
sim_scores_ik = ds.calculate_similarity(source1, target1)
df_out["cosine_scores_itemkeywords"]=sim_scores_ik

source2 = df["item_name"].loc[df['item_id'] == id].to_list()[0]
tdf2=df_out["item_name"].fillna('NA')
target2 = tdf2.to_list()
#print(target2)
sim_scores_in = ds.calculate_similarity(source2, target2)
df_out["cosine_scores_itemname"]=sim_scores_in

source = df["node_name"].loc[df['item_id'] == id].to_list()[0]
tdf=df_out["node_name"]+" " +df_out["product_type"].fillna('NA')
target = tdf.to_list()
# print(target)
sim_scores = ds.calculate_similarity(source, target)
# sim_scores

df_out["avg_cosine_score"] =  (df_out["cosine_scores_bulletpoint"]*0.8 + df_out["cosine_scores_itemkeywords"]*0.08 + df_out["cosine_scores_itemname"]*0.12) 
# df_out["avg_cosine_score"] =  (df_out["cosine_scores_bulletpoint"]+ df_out["cosine_scores_itemkeywords"]+ df_out["cosine_scores_itemname"])/3

df_out["eval_scores"]=sim_scores
df_out = df_out.sort_values('avg_cosine_score', ascending=False)

# df_out["eval_scores"]=sim_scores
df_out["error"]=abs((df_out["eval_scores"]-df_out["avg_cosine_score"]))
df_out=df_out.reset_index()
df_out["Rank"]=df_out.index+1

import seaborn as sns
sns.set(rc={"figure.figsize":(12, 8),'axes.titlesize':20,'axes.titleweight':'bold'}) 
ax=sns.lineplot(data=df_out.iloc[:100,:],x="Rank", y="avg_cosine_score",marker= 'o', markersize=6)
ax.set_xlabel ("Similarity Rank")
ax.set(ylim=(0.5, 1))
ax.set_ylabel ("Cosine Score")
ax.set_title ("Recommendation Rank vs Similarity Score")

# df_out[["Rank","item_id","item_name","node_name","cosine_scores","eval_scores","error"]]
df_out[["Rank","item_id","item_name","node_name","avg_cosine_score","eval_scores","error"]].round(2).head(50)

"""### Mean Absolute Error for the 10 recommendations"""

df_out["error"][:10].mean()

"""### Mean Absolute Error for the 50 recommendations"""

df_out["error"][:50].mean()

df[["item_id","item_name","item_keywords","node_name","bullet_point"]].loc[df['item_id'] == id].reset_index(drop=True)

